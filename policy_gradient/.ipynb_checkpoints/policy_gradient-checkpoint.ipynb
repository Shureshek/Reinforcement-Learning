{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVkCC1iri2SN"
   },
   "source": [
    "## HW 4: Policy gradient\n",
    "_Reference: based on Practical RL course by YSDA_\n",
    "\n",
    "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
    "\n",
    "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
    "\n",
    "\n",
    "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.718234Z",
     "start_time": "2025-09-28T12:13:27.715401Z"
    },
    "id": "7UYczVTli2Sb"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.803791Z",
     "start_time": "2025-09-28T12:13:27.733746Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "XPKYrIlai2Sf",
    "outputId": "2e044ee7-3baa-4bd7-a214-23b7225a88b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shurik\\anaconda3\\envs\\ml_1\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2220c991dc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75eHkuwTi2Si"
   },
   "source": [
    "# Building the network for Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_TFCmsWi2Sj"
   },
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.821515Z",
     "start_time": "2025-09-28T12:13:27.817340Z"
    },
    "id": "sY2THBWfi2Sl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.844491Z",
     "start_time": "2025-09-28T12:13:27.840982Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.857811Z",
     "start_time": "2025-09-28T12:13:27.855540Z"
    },
    "id": "8_pYr7PZi2Sn"
   },
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits.\n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "\n",
    "model = DQN(state_dim[0], n_actions)\n",
    "assert model is not None, \"model is not defined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.871193Z",
     "start_time": "2025-09-28T12:13:27.867165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_states_batch.shape: (5, 4)\n",
      "example_logits.shape: torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "# do not change the code block below\n",
    "batch_size_for_test = 5\n",
    "example_states_batch = np.array([env.reset()[0] for _ in range(5)])\n",
    "print(f\"example_states_batch.shape: {example_states_batch.shape}\")\n",
    "assert example_states_batch.shape == (batch_size_for_test, state_dim[0])\n",
    "\n",
    "example_logits = model(torch.from_numpy(example_states_batch))\n",
    "print(f\"example_logits.shape: {example_logits.shape}\")\n",
    "assert example_logits.shape == (batch_size_for_test, n_actions)\n",
    "# do not change the code block above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y80qbQFi2Sq"
   },
   "source": [
    "#### Predicting the action probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12PjRu0mi2Sr"
   },
   "source": [
    "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
    "\n",
    "So, here gradient calculation is not needed.\n",
    "\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "\n",
    "Also, `.detach()` can be used instead, but there is a difference:\n",
    "\n",
    "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.887714Z",
     "start_time": "2025-09-28T12:13:27.884208Z"
    },
    "id": "d5B5JuXCi2St"
   },
   "outputs": [],
   "source": [
    "def predict_probs(states, model):\n",
    "    \"\"\"\n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :param model: torch model\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "    with torch.no_grad():\n",
    "        states = torch.from_numpy(states).float()\n",
    "        logits = model(states)\n",
    "        probs = nn.functional.softmax(logits, dim=1).numpy()\n",
    "    assert probs is not None, \"probs is not defined\"\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.903402Z",
     "start_time": "2025-09-28T12:13:27.898898Z"
    },
    "id": "Obkl_jCii2Sv"
   },
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
    "test_probas = predict_probs(test_states, model)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be6AYf8gi2Sw"
   },
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.917037Z",
     "start_time": "2025-09-28T12:13:27.913734Z"
    },
    "id": "8LOUUvnki2Sx"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\"\n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]), model)[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, truncated, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.945765Z",
     "start_time": "2025-09-28T12:13:27.934583Z"
    },
    "id": "5sdENWJAi2Sz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.02467135, -0.01751848, -0.00533443,  0.048807  ], dtype=float32), array([-0.02502172, -0.21256354, -0.00435829,  0.3398021 ], dtype=float32), array([-0.02927299, -0.4076232 ,  0.00243775,  0.6311075 ], dtype=float32), array([-0.03742545, -0.21253535,  0.0150599 ,  0.33919328], dtype=float32), array([-0.04167616, -0.40786833,  0.02184377,  0.63658696], dtype=float32), array([-0.04983353, -0.603288  ,  0.03457551,  0.93606806], dtype=float32), array([-0.06189929, -0.79885876,  0.05329687,  1.2394121 ], dtype=float32), array([-0.07787646, -0.9946231 ,  0.07808511,  1.5483035 ], dtype=float32), array([-0.09776893, -0.8005205 ,  0.10905118,  1.2809705 ], dtype=float32), array([-0.11377934, -0.60694367,  0.13467059,  1.0243275 ], dtype=float32), array([-0.12591821, -0.8035769 ,  0.15515713,  1.3560804 ], dtype=float32), array([-0.14198975, -1.0002675 ,  0.18227875,  1.6930056 ], dtype=float32)] [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0] [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)\n",
    "print(states, actions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG5hLg-3i2S0"
   },
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.964262Z",
     "start_time": "2025-09-28T12:13:27.960849Z"
    },
    "id": "AoWX9gvai2S0"
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session\n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE\n",
    "    cumulative_rewards = []\n",
    "    g = 0\n",
    "    for r in rewards[::-1]:\n",
    "        g = g * gamma + r\n",
    "        cumulative_rewards.append(g)\n",
    "\n",
    "    cumulative_rewards.reverse()\n",
    "\n",
    "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
    "\n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.991019Z",
     "start_time": "2025-09-28T12:13:27.987517Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DX39wcUi2S3",
    "outputId": "9916590d-b093-4c5b-dd84-4ed9ed4b2ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evLt5DJji2S_"
   },
   "source": [
    "### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
    "\n",
    "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
    "\n",
    "$$\n",
    "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
    "$$\n",
    "where $\\lambda$ is the `entropy_coef`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:28.007546Z",
     "start_time": "2025-09-28T12:13:28.004036Z"
    },
    "id": "_hLjxTVLi2TB"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:28.028618Z",
     "start_time": "2025-09-28T12:13:28.024614Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Compute the loss for the REINFORCE algorithm.\n",
    "    \"\"\"\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    probs = nn.functional.softmax(logits, dim=1)\n",
    "    assert probs is not None, \"probs is not defined\"\n",
    "\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=1)\n",
    "    assert log_probs is not None, \"log_probs is not defined\"\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(log_probs * to_one_hot(actions, n_actions), dim=1)\n",
    "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
    "    J_hat = torch.mean(log_probs_for_actions * cumulative_returns)\n",
    "    assert J_hat is not None, \"J_hat is not defined\"\n",
    "    \n",
    "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1).mean()\n",
    "    assert entropy is not None, \"entropy is not defined\"\n",
    "    loss = -J_hat - entropy_coef * entropy\n",
    "    assert loss is not None, \"loss is not defined\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:28.041011Z",
     "start_time": "2025-09-28T12:13:28.037168Z"
    },
    "id": "1C8ZSizji2TD"
   },
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    logits = model(states)\n",
    "    # cast everything into torch tensors\n",
    "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-WWsbl5i2TE"
   },
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:14:10.594078Z",
     "start_time": "2025-09-28T12:13:28.051634Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckHj5sXBi2TE",
    "outputId": "017d3bcd-0d01-4632-ed1c-60642792cddf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shurik\\AppData\\Local\\Temp\\ipykernel_35460\\2359954175.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  states = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:19.750\n",
      "mean reward:39.750\n",
      "mean reward:85.970\n",
      "mean reward:117.940\n",
      "mean reward:19.620\n",
      "mean reward:30.830\n",
      "mean reward:349.280\n",
      "mean reward:369.820\n",
      "mean reward:222.340\n",
      "mean reward:495.680\n",
      "mean reward:144.340\n",
      "mean reward:264.110\n",
      "mean reward:250.610\n",
      "mean reward:83.480\n",
      "mean reward:128.250\n",
      "mean reward:967.730\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 800:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg__sQeti2TF"
   },
   "source": [
    "### Watch the video of your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:14:10.604173Z",
     "start_time": "2025-09-28T12:14:10.601660Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:14:12.460623Z",
     "start_time": "2025-09-28T12:14:10.616151Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'change_settings' from 'moviepy.config' (C:\\Users\\Shurik\\anaconda3\\envs\\ml_1\\Lib\\site-packages\\moviepy\\config.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msave_video\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_video\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmoviepy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m change_settings\n\u001b[32m      6\u001b[39m change_settings({\u001b[33m\"\u001b[39m\u001b[33mFFMPEG_BINARY\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mProgram Files\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mffmpeg\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbin\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mffmpeg.exe\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      8\u001b[39m env_for_video = gym.make(\u001b[33m\"\u001b[39m\u001b[33mCartPole-v1\u001b[39m\u001b[33m\"\u001b[39m, render_mode=\u001b[33m\"\u001b[39m\u001b[33mrgb_array_list\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'change_settings' from 'moviepy.config' (C:\\Users\\Shurik\\anaconda3\\envs\\ml_1\\Lib\\site-packages\\moviepy\\config.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from moviepy.config import change_settings\n",
    "change_settings({\"FFMPEG_BINARY\": r\"C:\\Program Files\\ffmpeg\\bin\\ffmpeg.exe\"})\n",
    "\n",
    "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "n_actions = env_for_video.action_space.n\n",
    "\n",
    "episode_index = 0\n",
    "step_starting_index = 0\n",
    "\n",
    "obs, info = env_for_video.reset()\n",
    "\n",
    "for step_index in range(800):\n",
    "    probs = predict_probs(np.array([obs]), model)[0]\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done or step_index == 799:\n",
    "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
    "        frames = env_for_video.render()\n",
    "        os.makedirs(\"videos\", exist_ok=True)\n",
    "        save_video(\n",
    "            frames, \"videos\",\n",
    "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
    "            step_starting_index=step_starting_index,\n",
    "            episode_index=episode_index,\n",
    "        )\n",
    "        episode_index += 1\n",
    "        step_starting_index = step_index + 1\n",
    "        obs, info = env_for_video.reset()\n",
    "\n",
    "env_for_video.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
