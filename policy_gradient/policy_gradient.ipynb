{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVkCC1iri2SN"
   },
   "source": [
    "## Policy Gradient REINFORCE Algorithm for CartPole-v1\n",
    "_Reference: based on Practical RL course by YSDA_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.718234Z",
     "start_time": "2025-09-28T12:13:27.715401Z"
    },
    "id": "7UYczVTli2Sb"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.821515Z",
     "start_time": "2025-09-28T12:13:27.817340Z"
    },
    "id": "sY2THBWfi2Sl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.803791Z",
     "start_time": "2025-09-28T12:13:27.733746Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "XPKYrIlai2Sf",
    "outputId": "2e044ee7-3baa-4bd7-a214-23b7225a88b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество действий: 2\n",
      "Размерность состояния: (4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shurik\\anaconda3\\envs\\ml_1\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x179f28e4620>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "print(f\"Количество действий: {n_actions}\")\n",
    "print(f\"Размерность состояния: {state_dim}\")\n",
    "\n",
    "plt.title(\"Начальное состояние среды CartPole\")\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75eHkuwTi2Si"
   },
   "source": [
    "# Архитектура Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_TFCmsWi2Sj"
   },
   "source": [
    "Для алгоритма REINFORCE нам потребуется модель, которая предсказывает вероятности действий в зависимости от состояния.\n",
    "Для обеспечения численной стабильности не используем softmax. Вынесем его отдельно, если понадобится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.844491Z",
     "start_time": "2025-09-28T12:13:27.840982Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Нейронная сеть для политики агента.\n",
    "    Возвращает логиты для каждого действия.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.857811Z",
     "start_time": "2025-09-28T12:13:27.855540Z"
    },
    "id": "8_pYr7PZi2Sn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNetwork(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = PolicyNetwork(state_dim[0], n_actions).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y80qbQFi2Sq"
   },
   "source": [
    "#### Функция предсказания вероятностей действий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12PjRu0mi2Sr"
   },
   "source": [
    "**Примечание:** возвращаемое значение этой функции — не torch-тензор, а numpy-массив.\n",
    "\n",
    "Поэтому вычисление градиентов здесь не требуется.\n",
    "\n",
    "Используйте no_grad, чтобы отключить вычисление градиентов.\n",
    "\n",
    "Также вместо этого можно использовать .detach(), но между ними есть разница:\n",
    "\n",
    "При использовании .detach() вычислительный граф строится, но затем отсоединяется от конкретного тензора. Поэтому .detach() следует применять в тех случаях, когда этот граф всё ещё нужен для обратного распространения градиента через другой (не отсоединённый) тензор.\n",
    "\n",
    "В отличие от этого, в контексте no_grad() вычислительный граф вообще не строится ни для одной операции, поэтому в данном случае предпочтительнее использовать именно его.\n",
    "\n",
    "Градиенты здесь не нужны, потому что:\n",
    "- это используется только для взаимодействия со средой\n",
    "- обучение будет позже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.887714Z",
     "start_time": "2025-09-28T12:13:27.884208Z"
    },
    "id": "d5B5JuXCi2St"
   },
   "outputs": [],
   "source": [
    "def predict_probs(states, model):\n",
    "    \"\"\"\n",
    "    Предсказывает вероятности действий для заданных состояний.\n",
    "    Args:\n",
    "        states: numpy array of shape [batch, state_shape]\n",
    "        model: torch model\n",
    "    Returns:\n",
    "        numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        logits = model(states)\n",
    "        # Применяем softmax для получения вероятностей\n",
    "        probs = nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be6AYf8gi2Sw"
   },
   "source": [
    "### Генерация эпизода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.917037Z",
     "start_time": "2025-09-28T12:13:27.913734Z"
    },
    "id": "8LOUUvnki2Sx"
   },
   "outputs": [],
   "source": [
    "def generate_episode(env, policy_model, t_max=1000):\n",
    "    \"\"\"\n",
    "    Генерирует один эпизод взаимодействия агента со средой.\n",
    "    \n",
    "    Args:\n",
    "        env: среда Gymnasium\n",
    "        policy_model: модель политики\n",
    "        max_steps: максимальное количество шагов\n",
    "        \n",
    "    Returns:\n",
    "        states: список состояний\n",
    "        actions: список действий\n",
    "        rewards: список наград\n",
    "    \"\"\"\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    # Сбрасываем среду\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Получаем вероятности действий для текущего состояния / pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]), policy_model)[0]\n",
    "\n",
    "        # Выбираем действие согласно политике\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "\n",
    "        # Выполняем действие в среде\n",
    "        new_s, r, done, truncated, info = env.step(a)\n",
    "\n",
    "        # Сохраняем переход\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.945765Z",
     "start_time": "2025-09-28T12:13:27.934583Z"
    },
    "id": "5sdENWJAi2Sz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.8804947e-02, 9.7163160e-05, 4.9638715e-02, 2.7332902e-03],\n",
      "      dtype=float32), array([ 0.01880689,  0.19447337,  0.04969338, -0.27388412], dtype=float32), array([ 0.02269636, -0.00132109,  0.0442157 ,  0.03404889], dtype=float32), array([ 0.02266994, -0.1970483 ,  0.04489668,  0.34034795], dtype=float32), array([ 0.01872897, -0.39277935,  0.05170364,  0.64684385], dtype=float32), array([ 0.01087338, -0.19841447,  0.06464051,  0.37088025], dtype=float32), array([ 0.00690509, -0.39439237,  0.07205812,  0.68322426], dtype=float32), array([-0.00098276, -0.20034109,  0.0857226 ,  0.4140694 ], dtype=float32), array([-0.00498958, -0.39656684,  0.09400399,  0.7324988 ], dtype=float32), array([-0.01292091, -0.59285337,  0.10865397,  1.0532252 ], dtype=float32), array([-0.02477798, -0.789235  ,  0.12971847,  1.377942  ], dtype=float32), array([-0.04056268, -0.9857163 ,  0.1572773 ,  1.7082181 ], dtype=float32), array([-0.06027701, -1.1822581 ,  0.19144167,  2.0454412 ], dtype=float32)] [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1] [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# тестовый запуск\n",
    "states, actions, rewards = generate_episode(env, model)\n",
    "print(states, actions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG5hLg-3i2S0"
   },
   "source": [
    "### Вычисление дисконтированных наград\n",
    "\n",
    "Чтобы работать с последовательными средами, нам необходимо знать накопленную дисконтированную награду для каждого состояния.\n",
    "Чтобы её вычислить, можно двигаться в обратном порядке — от конца эпизода к его началу — и считать дисконтированную накопленную награду следующим образом:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:27.964262Z",
     "start_time": "2025-09-28T12:13:27.960849Z"
    },
    "id": "AoWX9gvai2S0"
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Вычисляет дисконтированные совокупные награды (returns) для каждого шага.\n",
    "\n",
    "    Args:\n",
    "        rewards: список наград [r_0, r_1, ..., r_T]\n",
    "        gamma: коэффициент дисконтирования\n",
    "        \n",
    "    Returns:\n",
    "        cumulative_rewards: список дисконтированных наград\n",
    "    \"\"\"\n",
    "    \n",
    "    cumulative_rewards = []\n",
    "    g = 0\n",
    "\n",
    "    # Идем с конца эпизода к началу\n",
    "    for r in rewards[::-1]:\n",
    "        g = g * gamma + r\n",
    "        cumulative_rewards.append(g)\n",
    "\n",
    "    cumulative_rewards.reverse()\n",
    "\n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evLt5DJji2S_"
   },
   "source": [
    "### Функция потерь для REINFORCE\n",
    "\n",
    "Теперь нам нужно определить целевую функцию и правило обновления параметров с помощью градиента политики.\n",
    "\n",
    "Наша целевая функция имеет вид:\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "Алгоритм REINFORCE задаёт способ вычисления градиента ожидаемой награды по параметрам политики. Формула выглядит следующим образом:\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "Мы можем воспользоваться возможностями автоматического дифференцирования в PyTorch, определив целевую функцию в следующем виде:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "При вычислении градиента этой функции по весам сети $\\theta$ мы в точности получим градиент политики.\n",
    "\n",
    "Итоговая функция потерь также должна включать энтропийную регуляризацию $H(\\pi_\\theta (a_i \\mid s_i))$ , чтобы стимулировать исследование (exploration):\n",
    "\n",
    "$$\n",
    "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
    "$$\n",
    "где $\\lambda$ это коэффициент энтропийной регуляризации entropy_coef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:28.007546Z",
     "start_time": "2025-09-28T12:13:28.004036Z"
    },
    "id": "_hLjxTVLi2TB"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1).to(device)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:28.028618Z",
     "start_time": "2025-09-28T12:13:28.024614Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Вычисляет функцию потерь для алгоритма REINFORCE.\n",
    "    \n",
    "    Loss = -E[log π(a|s) * G] - β * H(π)\n",
    "    где H(π) — энтропия политики (для исследования)\n",
    "    \n",
    "    Args:\n",
    "        logits: выход нейронной сети [batch, n_actions]\n",
    "        actions: выбранные действия [batch]\n",
    "        rewards: дисконтированные награды [batch]\n",
    "        entropy_coef: коэффициент энтропии\n",
    "        \n",
    "    Returns:\n",
    "        loss: значение функции потерь\n",
    "    \"\"\"\n",
    "    actions = torch.tensor(actions, dtype=torch.int32).to(device)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Вычисляем вероятности и логарифмы вероятностей\n",
    "    probs = nn.functional.softmax(logits, dim=1)\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=1)\n",
    "\n",
    "    # Логарифмы вероятностей для выбранных действий\n",
    "    log_probs_for_actions = torch.sum(log_probs * to_one_hot(actions, n_actions), dim=1)\n",
    "\n",
    "    # Основная часть loss: log π(a|s) * G\n",
    "    J_hat = torch.mean(log_probs_for_actions * cumulative_returns)\n",
    "    \n",
    "    # Энтропия для исследования: -Σ p * log p\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1).mean()\n",
    "\n",
    "    # Итоговый loss\n",
    "    loss = -J_hat - entropy_coef * entropy\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:13:28.041011Z",
     "start_time": "2025-09-28T12:13:28.037168Z"
    },
    "id": "1C8ZSizji2TD"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Обучает агента на одном эпизоде (session), сгенерированном текущей политикой.\n",
    "    Функция:\n",
    "    1) вычисляет логиты политики для всех состояний эпизода,\n",
    "    2) считает функцию потерь по формуле REINFORCE,\n",
    "    3) делает шаг градиентного спуска,\n",
    "    4) возвращает суммарную награду эпизода (для логирования).\n",
    "    \n",
    "    Args:\n",
    "    - states  — список состояний, посещённых агентом в эпизоде\n",
    "    - actions — список действий, выбранных агентом в этих состояниях\n",
    "    - rewards — список наград, полученных агентом на каждом шаге\n",
    "    - gamma — коэффициент дисконтирования будущих наград\n",
    "    - entropy_coef — коэффициент энтропийной регуляризации (стимулирует exploration)\n",
    "    \"\"\"\n",
    "\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Прогоняем все состояния через модель политики\n",
    "    # На выходе получаем логиты действий для каждого состояния\n",
    "    # shape: [T, n_actions]\n",
    "    logits = model(states)\n",
    "    \n",
    "    # Вычисляем функцию потерь REINFORCE\n",
    "    # Внутри:\n",
    "    # - считается cumulative return G_t\n",
    "    # - выбираются log π(a_t | s_t)\n",
    "    # - добавляется энтропийная регуляризация\n",
    "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Возвращаем суммарную награду за эпизод\n",
    "    # Используется только для мониторинга обучения\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:14:10.594078Z",
     "start_time": "2025-09-28T12:13:28.051634Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckHj5sXBi2TE",
    "outputId": "017d3bcd-0d01-4632-ed1c-60642792cddf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shurik\\AppData\\Local\\Temp\\ipykernel_21560\\3271327488.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  states = torch.tensor(states, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:37.950\n",
      "mean reward:58.090\n",
      "mean reward:84.030\n",
      "mean reward:155.740\n",
      "mean reward:250.800\n",
      "mean reward:71.700\n",
      "mean reward:199.100\n",
      "mean reward:217.220\n",
      "mean reward:118.270\n",
      "mean reward:328.080\n",
      "mean reward:211.340\n",
      "mean reward:215.840\n",
      "mean reward:167.020\n",
      "mean reward:127.190\n",
      "mean reward:153.310\n",
      "mean reward:152.120\n",
      "mean reward:46.450\n",
      "mean reward:48.550\n",
      "mean reward:104.430\n",
      "mean reward:120.680\n",
      "mean reward:170.310\n",
      "mean reward:128.190\n",
      "mean reward:105.160\n",
      "mean reward:95.430\n",
      "mean reward:85.740\n",
      "mean reward:57.980\n",
      "mean reward:33.220\n",
      "mean reward:132.380\n",
      "mean reward:188.680\n",
      "mean reward:155.330\n",
      "mean reward:171.500\n",
      "mean reward:197.480\n",
      "mean reward:736.960\n",
      "mean reward:307.760\n",
      "mean reward:141.530\n",
      "mean reward:109.230\n",
      "mean reward:94.270\n",
      "mean reward:29.870\n",
      "mean reward:100.290\n",
      "mean reward:86.500\n",
      "mean reward:157.150\n",
      "mean reward:171.120\n",
      "mean reward:235.600\n",
      "mean reward:249.580\n",
      "mean reward:682.730\n",
      "mean reward:1000.000\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    rewards = [train_on_session(*generate_episode(env, model), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 800:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg__sQeti2TF"
   },
   "source": [
    "### Визуализация результатов обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T12:14:12.460623Z",
     "start_time": "2025-09-28T12:14:10.616151Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_video() missing 1 required positional argument: 'video_folder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done:  \u001b[38;5;66;03m# сохраняем видео только когда эпизод реально завершён\u001b[39;00m\n\u001b[32m     22\u001b[39m     frames = env_for_video.render()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[43msave_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     episode_index += \u001b[32m1\u001b[39m\n\u001b[32m     25\u001b[39m     step_starting_index = step_index + \u001b[32m1\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: save_video() missing 1 required positional argument: 'video_folder'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "n_actions = env_for_video.action_space.n\n",
    "\n",
    "episode_index = 0\n",
    "step_starting_index = 0\n",
    "\n",
    "obs, info = env_for_video.reset()\n",
    "\n",
    "for step_index in range(800):\n",
    "    probs = predict_probs(np.array([obs]), model)[0]\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done or step_index == 799:\n",
    "        frames = env_for_video.render()\n",
    "        os.makedirs(\"videos\", exist_ok=True)\n",
    "        save_video(\n",
    "            frames, \"videos\",\n",
    "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
    "            step_starting_index=step_starting_index,\n",
    "            episode_index=episode_index,\n",
    "        )\n",
    "        episode_index += 1\n",
    "        step_starting_index = step_index + 1\n",
    "        obs, info = env_for_video.reset()\n",
    "\n",
    "\n",
    "        save_video(...)\n",
    "        episode_index += 1\n",
    "        step_starting_index = step_index + 1\n",
    "        obs, info = env_for_video.reset()\n",
    "\n",
    "env_for_video.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
