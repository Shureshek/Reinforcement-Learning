{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVkCC1iri2SN"
   },
   "source": [
    "## HW 4: Policy gradient\n",
    "_Reference: based on Practical RL course by YSDA_\n",
    "\n",
    "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
    "\n",
    "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
    "\n",
    "\n",
    "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7UYczVTli2Sb",
    "ExecuteTime": {
     "end_time": "2025-09-28T05:30:49.609986Z",
     "start_time": "2025-09-28T05:30:49.606986Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "XPKYrIlai2Sf",
    "outputId": "2e044ee7-3baa-4bd7-a214-23b7225a88b5",
    "ExecuteTime": {
     "end_time": "2025-09-28T05:30:49.688765Z",
     "start_time": "2025-09-28T05:30:49.618706Z"
    }
   },
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29936c61010>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75eHkuwTi2Si"
   },
   "source": [
    "# Building the network for Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_TFCmsWi2Sj"
   },
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sY2THBWfi2Sl",
    "ExecuteTime": {
     "end_time": "2025-09-28T05:30:49.704383Z",
     "start_time": "2025-09-28T05:30:49.700869Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T05:30:49.731449Z",
     "start_time": "2025-09-28T05:30:49.728178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8_pYr7PZi2Sn",
    "ExecuteTime": {
     "end_time": "2025-09-28T05:30:49.754620Z",
     "start_time": "2025-09-28T05:30:49.751004Z"
    }
   },
   "source": [
    "# Build a simple neural network that predicts policy logits.\n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "\n",
    "model = DQN(state_dim[0], n_actions)\n",
    "assert model is not None, \"model is not defined\""
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T05:30:50.739261Z",
     "start_time": "2025-09-28T05:30:50.720198Z"
    }
   },
   "source": [
    "# do not change the code block below\n",
    "batch_size_for_test = 5\n",
    "example_states_batch = np.array([env.reset()[0] for _ in range(5)])\n",
    "print(f\"example_states_batch.shape: {example_states_batch.shape}\")\n",
    "assert example_states_batch.shape == (batch_size_for_test, state_dim[0])\n",
    "\n",
    "example_logits = model(torch.from_numpy(example_states_batch))\n",
    "print(f\"example_logits.shape: {example_logits.shape}\")\n",
    "assert example_logits.shape == (batch_size_for_test, n_actions)\n",
    "# do not change the code block above"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_states_batch.shape: (5, 4)\n",
      "example_logits.shape: torch.Size([5, 2])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y80qbQFi2Sq"
   },
   "source": [
    "#### Predicting the action probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12PjRu0mi2Sr"
   },
   "source": [
    "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
    "\n",
    "So, here gradient calculation is not needed.\n",
    "\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "\n",
    "Also, `.detach()` can be used instead, but there is a difference:\n",
    "\n",
    "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5B5JuXCi2St"
   },
   "outputs": [],
   "source": [
    "def predict_probs(states, model):\n",
    "    \"\"\"\n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :param model: torch model\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    states = torch.from_numpy(states).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        probs = model(states)\n",
    "    assert probs is not None, \"probs is not defined\"\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Obkl_jCii2Sv"
   },
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be6AYf8gi2Sw"
   },
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LOUUvnki2Sx"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\"\n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, truncated, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sdENWJAi2Sz"
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG5hLg-3i2S0"
   },
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoWX9gvai2S0"
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session\n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE\n",
    "    cumulative_rewards = None\n",
    "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
    "\n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DX39wcUi2S3",
    "outputId": "9916590d-b093-4c5b-dd84-4ed9ed4b2ba7"
   },
   "outputs": [],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evLt5DJji2S_"
   },
   "source": [
    "### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
    "\n",
    "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
    "\n",
    "$$\n",
    "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
    "$$\n",
    "where $\\lambda$ is the `entropy_coef`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hLjxTVLi2TB"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Compute the loss for the REINFORCE algorithm.\n",
    "    \"\"\"\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    probs = None\n",
    "    assert probs is not None, \"probs is not defined\"\n",
    "\n",
    "    log_probs = None\n",
    "    assert log_probs is not None, \"log_probs is not defined\"\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = None # [batch,]\n",
    "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
    "    J_hat = None  # a number\n",
    "    assert J_hat is not None, \"J_hat is not defined\"\n",
    "    \n",
    "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
    "    entropy = None\n",
    "    assert entropy is not None, \"entropy is not defined\"\n",
    "    loss = None\n",
    "    assert loss is not None, \"loss is not defined\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1C8ZSizji2TD"
   },
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    logits = model(states)\n",
    "    # cast everything into torch tensors\n",
    "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-WWsbl5i2TE"
   },
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckHj5sXBi2TE",
    "outputId": "017d3bcd-0d01-4632-ed1c-60642792cddf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 800:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg__sQeti2TF"
   },
   "source": [
    "### Watch the video of your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "n_actions = env_for_video.action_space.n\n",
    "\n",
    "episode_index = 0\n",
    "step_starting_index = 0\n",
    "\n",
    "obs, info = env_for_video.reset()\n",
    "\n",
    "for step_index in range(800):\n",
    "    probs = predict_probs(np.array([obs]))[0]\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done or step_index == 799:\n",
    "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
    "        frames = env_for_video.render()\n",
    "        os.makedirs(\"videos\", exist_ok=True)\n",
    "        save_video(\n",
    "            frames, \"videos\",\n",
    "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
    "            step_starting_index=step_starting_index,\n",
    "            episode_index=episode_index,\n",
    "        )\n",
    "        episode_index += 1\n",
    "        step_starting_index = step_index + 1\n",
    "        obs, info = env_for_video.reset()\n",
    "\n",
    "env_for_video.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus part (no points, just for the interested ones)\n",
    "\n",
    "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
    "\n",
    "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your brave and victorious code here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py3_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
